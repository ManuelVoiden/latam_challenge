{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data Engineer LATAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook es necesario tener los datos listos, teniendo en cuenta las restricciones de tamaño de GitHub los tweets ha evaluar se han subido en formato .zip.\n",
    "\n",
    "Se ha creado una función que permite extraer este archivo .zip ejecutando el siguiente comando:\n",
    "\n",
    "´´´\n",
    "    python .\\src\\zip_extraction.py \n",
    "´´´\n",
    "\n",
    "Esto dejará disponible el archivo .json con los tweets a evaluar dentro de la carpeta data y listos para lectura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establecer variable con la ruta de los tweets a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import memory_profiler\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "#import concurrent\n",
    "\n",
    "from datetime import datetime\n",
    "from memory_profiler import memory_usage\n",
    "from collections import defaultdict, Counter\n",
    "#from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Time and Memory solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera versión de Q1 Time usando Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera aproximación a la solución fue con Pandas, leer el archivo completo y cargarlo en un dataframe, de aquí lo que se hace es agrupar por fecha y luego con .agg contar y ordenar los tweets por fecha y usuario, finalmente se hace un sort para ordenar los datos, se convierte el formato solicitado y se retorna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_pandas(file_path: str):\n",
    "    # Leer el JSON y cargarlo a un dataframe de pandas\n",
    "    tweets_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Convertir la columna 'date' a datetime\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date']).dt.date\n",
    "\n",
    "    # Agrupar por fecha y contar la cantidad de tweets por fecha y usuario\n",
    "    top_dates_df = tweets_df.groupby('date').agg(\n",
    "        total_tweets=('date', 'size'),\n",
    "        most_active_user=('user', lambda x: x.mode()[0]['username'])\n",
    "    ).sort_values('total_tweets', ascending=False).head(10)\n",
    "\n",
    "    # Convertir el dataframe a una lista de tuplas\n",
    "    top_dates = [(row.name, row['most_active_user']) for _, row in top_dates_df.iterrows()]\n",
    "\n",
    "    return top_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las pruebas de memoria y tiempo estaban dando como resultado entre 33 y 35 segundos de tiempo de ejecución y un consumo maximo de memoria de entre 2950MiB y 3100MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "q1_memory - Execution Time: 33.76640963554382 seconds\n",
      "q1_memory - Peak Memory Usage: 2968.328125 MiB\n"
     ]
    }
   ],
   "source": [
    "#Ejecutar la función y mostrar el resultado\n",
    "top_dates = q1_time_pandas(file_path)\n",
    "print(top_dates)\n",
    "\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q1_time_pandas, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q1_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q1_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda versión de Q1 Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La modificación clave en esta versión definitiva de Q1 Time es la extracción de los usuarios y fecha, dejando de lado el resto de las columnas antes de realizar el agrupamiento, así mismo el output en tuplas se realiza de una manera diferente convertiendo cada par de fecha, usuario en una tupla y estas luego en una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_pandas_revised(file_path: str):\n",
    "    # Leer el JSON y cargarlo a un dataframe de pandas\n",
    "    tweets_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Convertir la columna 'date' a datetime\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date']).dt.date\n",
    "\n",
    "    # Extraer el username que esta anidado en la columna 'user'\n",
    "    tweets_df['username'] = tweets_df['user'].apply(lambda x: x['username'])\n",
    "\n",
    "    # Agrupar por fecha y contar la cantidad de tweets por fecha y usuario\n",
    "    top_dates_df = tweets_df.groupby('date').agg(\n",
    "        total_tweets=('date', 'size'),\n",
    "        most_active_user=('username', lambda x: x.mode()[0])\n",
    "    ).sort_values('total_tweets', ascending=False).head(10).drop(columns=['total_tweets'])\n",
    "\n",
    "    # Convertir el dataframe a una lista de tuplas\n",
    "    top_dates = list(top_dates_df.itertuples(index=True, name=None))\n",
    "\n",
    "    return top_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo tiempo de ejecución con estas modificaciones es de entre 7.5 y 9 segundos manteniendo el uso máximo en memoria.\n",
    "Una reducción de alrededor del 75% en tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "q1_time_pandas_revised - Execution Time: 7.851444482803345 seconds\n",
      "q1_time_pandas_revised - Peak Memory Usage: 2991.9140625 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función y mostrar el resultado\n",
    "top_dates = q1_time_pandas_revised(file_path)\n",
    "print(top_dates)\n",
    "\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_time = max(memory_usage((q1_time_pandas_revised, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q1_time_pandas_revised - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q1_time_pandas_revised - Peak Memory Usage: {peak_memory_time} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Memory usando una aproximación de lectura linea por linea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La aproximación para Q1 Memory es la de no cargar el archivo en memoria si nó mas bien leer linea por linea, algo similar a como se manejan datos en streaming, con la libreria JSON se puede conseguir el resultado esperado.\n",
    "\n",
    "Esta aproximación al problema no solo disminuye el gasto maximo en memoria a alrededor de 100MiB si nó que también es mucho más rapido en ejecución, entre 3.7 y 4.2s segundos, esto ya que estamos tratando una serie de datos relativamente pequeños, lo que permite que esta aproximación de optimización de memoria se la solución mas eficiente en tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str):\n",
    "    tweet_counts = Counter()\n",
    "    user_activity = defaultdict(Counter)\n",
    "\n",
    "    # Leer el JSON fila por fila\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            date = datetime.fromisoformat(tweet['date']).date()\n",
    "            username = tweet['user']['username']\n",
    "\n",
    "            # Contar la cantidad de tweets por fecha y usuario\n",
    "            tweet_counts[date] += 1\n",
    "            user_activity[date][username] += 1\n",
    "\n",
    "    # Sacar el top 10 de fechas con mas tweets\n",
    "    top_dates = tweet_counts.most_common(10)\n",
    "\n",
    "    # Sacar el usuario mas activo por fecha\n",
    "    top_dates = [(date, user_activity[date].most_common(1)[0][0]) for date, _ in top_dates]\n",
    "\n",
    "    return top_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 Memory con la lectura linea por linea presenta un tiempo de ejecución de entre 3.7s a 4.2s y un uso máximo de memoria de entre 70MiB y 120MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "q1_memory - Execution Time: 3.816939115524292 seconds\n",
      "q1_memory - Peak Memory Usage: 59.87109375 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función y mostrar el resultado\n",
    "top_dates = q1_memory(file_path)\n",
    "print(top_dates)\n",
    "\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q1_memory, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q1_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q1_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Time and Memory solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Time con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproximación inicial a Q2 Time con Pandas usando lo aprendido en la Q1 Time, usando REGEX para encontrar los emojis, se carga el JSON, se convierte a df, con REFEX se encuentran los Emojis y luego se ordena para encontrar los mas usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_pandas(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Definir los emojis que se van a buscar\n",
    "    emoji_pattern = re.compile('[\\U0001F600-\\U0001F64F]')\n",
    "\n",
    "    # Cargar los tweets a un dataframe de pandas\n",
    "    tweets_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Extraer los emojis de la columna 'content'\n",
    "    tweets_df['emojis'] = tweets_df['content'].apply(lambda x: emoji_pattern.findall(x))\n",
    "\n",
    "    # Expandir la columna 'emojis' para que cada fila tenga un solo emoji con .explode()\n",
    "    df_emojis = tweets_df.explode('emojis')\n",
    "\n",
    "    # Traer los 10 emojis mas usados\n",
    "    top_emojis = df_emojis['emojis'].value_counts().head(10)\n",
    "\n",
    "    return top_emojis.reset_index().rename(columns={'index': 'emoji', 'emojis': 'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 Time con Pandas estaba dando en los tests entre 8.5s y 9.5s de tiempo de ejecución, con cerca de 3200MiB de punto maximo de consumo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  count\n",
      "0      🙏   7286\n",
      "1      😂   3072\n",
      "2      😡    378\n",
      "3      😁    280\n",
      "4      😊    259\n",
      "5      😢    225\n",
      "6      🙄    219\n",
      "7      🙌    215\n",
      "8      😀    213\n",
      "9      😜    202\n",
      "q2_pandas - Execution Time: 8.545934915542603 seconds\n",
      "q2_pandas - Peak Memory Usage: 3149.39453125 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función y mostrar el resultado\n",
    "top_emojis_df = q2_pandas(file_path)\n",
    "print(top_emojis_df)\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q2_pandas, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q2_pandas - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q2_pandas - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Time, dejando de usar pandas, lectura del archivo completo con la libreria json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se buscó otro angulo para Q2 Time buscando reducir el tiempo de ejecución, reemplacé la carga de la información con Pandas a cargar el archivo y leerlo con la liberia de JSON linea a linea, similar a como se hizo en Q1 Memory pero con el archivo cargado desde antes buscando tener un menor teimpo de ejecución, así mismo se usa .findall para encontrar los emojis despues de extraer el contenido del tweet de el apartado de content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "    emoji_pattern = re.compile('[\\U0001F600-\\U0001F64F]')  #REGEX para encontrar emojis\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.readlines()\n",
    "        #Carga el json y busca los emojis en cada tweet\n",
    "\n",
    "    for line in data:\n",
    "        tweet = json.loads(line)\n",
    "        content = tweet.get('content', '')\n",
    "        emojis = emoji_pattern.findall(content)\n",
    "        emoji_counter.update(emojis)\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cambio de angulo a Q2 Time ayudó considerablemente en la reducción en el tiempo de ejecución, ahora entre 3.7s y 4s y adicional con una considerable reducción en el pico mas alto de memoria, teniendo como promedio 600MiB versus los 3000MiB de la solución con Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286), ('😂', 3072), ('😡', 378), ('😁', 280), ('😊', 259), ('😢', 225), ('🙄', 219), ('🙌', 215), ('😀', 213), ('😜', 202)]\n",
      "q2_time - Execution Time: 3.7877962589263916 seconds\n",
      "q2_time - Peak Memory Usage: 605.49609375 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función y mostrar el resultado\n",
    "top_emojis = q2_time(file_path)\n",
    "print(top_emojis)\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q2_time, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q2_time - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q2_time - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Memory evitando cargar el archivo completo, se usa lectura linea por linea para buscar los emojis por cada tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución Q2 Memory tomé un angulo muy similar al de la solución final de Q2 Time pero en vez de cargar el archivo JSON completamente se hace lo mismo que en Q1 Memory leyendo el archivo fila a fila, la logíca base es la misma que en Q2 Time pero este cambio logra reducir el pico de memoria de la función a un promedio 550MiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "    emoji_pattern = re.compile('[\\U0001F600-\\U0001F64F]')  #REGEX para encontrar emojis\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            content = tweet.get('content', '')\n",
    "            emojis = emoji_pattern.findall(content)\n",
    "            for emoji in emojis:\n",
    "                emoji_counter[emoji] += 1\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al analizar los tests de eficiencia en tiempo y memoria podemos ver que el tiempo de ejecución es ligeramente mas alto que en Q2 Time, pero se encuentra una reducción en el pico de memoria, de un promedio de 600MiB a 550MiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286), ('😂', 3072), ('😡', 378), ('😁', 280), ('😊', 259), ('😢', 225), ('🙄', 219), ('🙌', 215), ('😀', 213), ('😜', 202)]\n",
      "q2_memory - Execution Time: 3.8695602416992188 seconds\n",
      "q2_memory - Peak Memory Usage: 551.01953125 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función y mostrar el resultado\n",
    "top_emojis = q2_memory(file_path)\n",
    "print(top_emojis)\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q2_memory, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q2_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q2_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Time and Memory solucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución de Q3 Time y Q3 Memory comparte mucho del desarrollo de Q2, para variar las soluciones elegí irme con Pandas + REGEX para Q3 Time, con REGEX se puede encontrar las menciones a distintos usuarios para luego guardar los usernames en un dataframe, se reutiliza el .explode que ya se habia explorado en Q2 y se ordena por usuarios mas mencionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Time solucion con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Read the entire file into a Pandas DataFrame\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Regular expression to extract mentions\n",
    "    mention_pattern = re.compile(r'@(\\w+)')\n",
    "\n",
    "    # Extract mentions and explode the DataFrame\n",
    "    df['mentions'] = df['content'].str.findall(mention_pattern)\n",
    "    df_exploded = df.explode('mentions')\n",
    "\n",
    "    # Count and get the top 10 mentions\n",
    "    top_mentions = df_exploded['mentions'].value_counts().head(10)\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    top_mentions_output = list(top_mentions.items())\n",
    "\n",
    "    return top_mentions_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El analisis de los tests de Q3 muestra un tiempo promedio de ejecución de 9.5s y un pico de uso de memoria de 3400MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1639), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "q3_time - Execution Time: 9.334634780883789 seconds\n",
      "q3_time - Peak Memory Usage: 3332.78125 MiB\n"
     ]
    }
   ],
   "source": [
    "#Executing q3_time and printing the result\n",
    "top_mentions = q3_time(file_path)\n",
    "print(top_mentions)\n",
    "# Timing and memory testing for q1_memory\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q3_time, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q3_time - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q3_time - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Memory solucion, usa el metodo de lectura linea por linea del JSON usado anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución de Q3 Memory se usa una combinación de REGEX y de lectura fila a fila del JSON de tweets, con una estructura similar a Q2 Memory donde se busca con .findall las menciones dentro de la columna de content, esta solución reduce en un 45% el pico de consumo de memoria y como se mencionó en Q1 ya que la cantidad de tweets procesados no es lo suficientemente alta como para que una lectura linea a linea sea mas lenta que una carga inicial de los datos completos el tiempo de ejecución también se reduce, de 9s en promedio a 4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "    mention_pattern = re.compile(r'@(\\w+)')\n",
    "\n",
    "    # Process file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            mentions = mention_pattern.findall(tweet.get('content', ''))\n",
    "            for mention in mentions:\n",
    "                mention_counter[mention] += 1\n",
    "\n",
    "    # Get the top 10 mentions\n",
    "    top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1639), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "q3_memory - Execution Time: 3.8050408363342285 seconds\n",
      "q3_memory - Peak Memory Usage: 1906.46484375 MiB\n"
     ]
    }
   ],
   "source": [
    "#Executing q3_memory and printing the result\n",
    "top_mentions = q3_memory(file_path)\n",
    "print(top_mentions)\n",
    "# Timing and memory testing for q1_memory\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q3_memory, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q3_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q3_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
