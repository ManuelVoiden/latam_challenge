{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data Engineer LATAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook es necesario tener los datos listos, teniendo en cuenta las restricciones de tama帽o de GitHub los tweets ha evaluar se han subido en formato .zip.\n",
    "\n",
    "Se ha creado una funci贸n que permite extraer este archivo .zip ejecutando el siguiente comando:\n",
    "\n",
    "麓麓麓\n",
    "    python .\\src\\zip_extraction.py \n",
    "麓麓麓\n",
    "\n",
    "Esto dejar谩 disponible el archivo .json con los tweets a evaluar dentro de la carpeta data y listos para lectura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establecer variable con la ruta de los tweets a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import memory_profiler\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "#import concurrent\n",
    "\n",
    "from datetime import datetime\n",
    "from memory_profiler import memory_usage\n",
    "from collections import defaultdict, Counter\n",
    "#from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Time and Memory soluci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera versi贸n de Q1 Time usando Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera aproximaci贸n a la soluci贸n fue con Pandas, leer el archivo completo y cargarlo en un dataframe, de aqu铆 lo que se hace es agrupar por fecha y luego con .agg contar y ordenar los tweets por fecha y usuario, finalmente se hace un sort para ordenar los datos, se convierte el formato solicitado y se retorna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_pandas(file_path: str):\n",
    "    # Leer el JSON y cargarlo a un dataframe de pandas\n",
    "    tweets_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Convertir la columna 'date' a datetime\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date']).dt.date\n",
    "\n",
    "    # Agrupar por fecha y contar la cantidad de tweets por fecha y usuario\n",
    "    top_dates_df = tweets_df.groupby('date').agg(\n",
    "        total_tweets=('date', 'size'),\n",
    "        most_active_user=('user', lambda x: x.mode()[0]['username'])\n",
    "    ).sort_values('total_tweets', ascending=False).head(10)\n",
    "\n",
    "    # Convertir el dataframe a una lista de tuplas\n",
    "    top_dates = [(row.name, row['most_active_user']) for _, row in top_dates_df.iterrows()]\n",
    "\n",
    "    return top_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las pruebas de memoria y tiempo estaban dando como resultado entre 33 y 35 segundos de tiempo de ejecuci贸n y un consumo maximo de memoria de entre 2950MiB y 3100MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "q1_memory - Execution Time: 33.76640963554382 seconds\n",
      "q1_memory - Peak Memory Usage: 2968.328125 MiB\n"
     ]
    }
   ],
   "source": [
    "#Ejecutar la funci贸n y mostrar el resultado\n",
    "top_dates = q1_time_pandas(file_path)\n",
    "print(top_dates)\n",
    "\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q1_time_pandas, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q1_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q1_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda versi贸n de Q1 Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La modificaci贸n clave en esta versi贸n definitiva de Q1 Time es la extracci贸n de los usuarios y fecha, dejando de lado el resto de las columnas antes de realizar el agrupamiento, as铆 mismo el output en tuplas se realiza de una manera diferente convertiendo cada par de fecha, usuario en una tupla y estas luego en una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_pandas_revised(file_path: str):\n",
    "    # Leer el JSON y cargarlo a un dataframe de pandas\n",
    "    tweets_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Convertir la columna 'date' a datetime\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date']).dt.date\n",
    "\n",
    "    # Extraer el username que esta anidado en la columna 'user'\n",
    "    tweets_df['username'] = tweets_df['user'].apply(lambda x: x['username'])\n",
    "\n",
    "    # Agrupar por fecha y contar la cantidad de tweets por fecha y usuario\n",
    "    top_dates_df = tweets_df.groupby('date').agg(\n",
    "        total_tweets=('date', 'size'),\n",
    "        most_active_user=('username', lambda x: x.mode()[0])\n",
    "    ).sort_values('total_tweets', ascending=False).head(10).drop(columns=['total_tweets'])\n",
    "\n",
    "    # Convertir el dataframe a una lista de tuplas\n",
    "    top_dates = list(top_dates_df.itertuples(index=True, name=None))\n",
    "\n",
    "    return top_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo tiempo de ejecuci贸n con estas modificaciones es de entre 7.5 y 9 segundos manteniendo el uso m谩ximo en memoria.\n",
    "Una reducci贸n de alrededor del 75% en tiempos de ejecuci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "q1_time_pandas_revised - Execution Time: 7.851444482803345 seconds\n",
      "q1_time_pandas_revised - Peak Memory Usage: 2991.9140625 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la funci贸n y mostrar el resultado\n",
    "top_dates = q1_time_pandas_revised(file_path)\n",
    "print(top_dates)\n",
    "\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_time = max(memory_usage((q1_time_pandas_revised, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q1_time_pandas_revised - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q1_time_pandas_revised - Peak Memory Usage: {peak_memory_time} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Memory usando una aproximaci贸n de lectura linea por linea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La aproximaci贸n para Q1 Memory es la de no cargar el archivo en memoria si n贸 mas bien leer linea por linea, algo similar a como se manejan datos en streaming, con la libreria JSON se puede conseguir el resultado esperado.\n",
    "\n",
    "Esta aproximaci贸n al problema no solo disminuye el gasto maximo en memoria a alrededor de 100MiB si n贸 que tambi茅n es mucho m谩s rapido en ejecuci贸n, entre 3.7 y 4.2s segundos, esto ya que estamos tratando una serie de datos relativamente peque帽os, lo que permite que esta aproximaci贸n de optimizaci贸n de memoria se la soluci贸n mas eficiente en tiempo de ejecuci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str):\n",
    "    tweet_counts = Counter()\n",
    "    user_activity = defaultdict(Counter)\n",
    "\n",
    "    # Leer el JSON fila por fila\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            date = datetime.fromisoformat(tweet['date']).date()\n",
    "            username = tweet['user']['username']\n",
    "\n",
    "            # Contar la cantidad de tweets por fecha y usuario\n",
    "            tweet_counts[date] += 1\n",
    "            user_activity[date][username] += 1\n",
    "\n",
    "    # Sacar el top 10 de fechas con mas tweets\n",
    "    top_dates = tweet_counts.most_common(10)\n",
    "\n",
    "    # Sacar el usuario mas activo por fecha\n",
    "    top_dates = [(date, user_activity[date].most_common(1)[0][0]) for date, _ in top_dates]\n",
    "\n",
    "    return top_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 Memory con la lectura linea por linea presenta un tiempo de ejecuci贸n de entre 3.7s a 4.2s y un uso m谩ximo de memoria de entre 70MiB y 120MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "q1_memory - Execution Time: 3.816939115524292 seconds\n",
      "q1_memory - Peak Memory Usage: 59.87109375 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la funci贸n y mostrar el resultado\n",
    "top_dates = q1_memory(file_path)\n",
    "print(top_dates)\n",
    "\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q1_memory, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q1_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q1_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Time and Memory soluci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Time con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproximaci贸n inicial a Q2 Time con Pandas usando lo aprendido en la Q1 Time, usando REGEX para encontrar los emojis, se carga el JSON, se convierte a df, con REFEX se encuentran los Emojis y luego se ordena para encontrar los mas usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_pandas(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Definir los emojis que se van a buscar\n",
    "    emoji_pattern = re.compile('[\\U0001F600-\\U0001F64F]')\n",
    "\n",
    "    # Cargar los tweets a un dataframe de pandas\n",
    "    tweets_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Extraer los emojis de la columna 'content'\n",
    "    tweets_df['emojis'] = tweets_df['content'].apply(lambda x: emoji_pattern.findall(x))\n",
    "\n",
    "    # Expandir la columna 'emojis' para que cada fila tenga un solo emoji con .explode()\n",
    "    df_emojis = tweets_df.explode('emojis')\n",
    "\n",
    "    # Traer los 10 emojis mas usados\n",
    "    top_emojis = df_emojis['emojis'].value_counts().head(10)\n",
    "\n",
    "    return top_emojis.reset_index().rename(columns={'index': 'emoji', 'emojis': 'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 Time con Pandas estaba dando en los tests entre 8.5s y 9.5s de tiempo de ejecuci贸n, con cerca de 3200MiB de punto maximo de consumo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  count\n",
      "0         7286\n",
      "1         3072\n",
      "2          378\n",
      "3          280\n",
      "4          259\n",
      "5          225\n",
      "6          219\n",
      "7          215\n",
      "8          213\n",
      "9          202\n",
      "q2_pandas - Execution Time: 8.545934915542603 seconds\n",
      "q2_pandas - Peak Memory Usage: 3149.39453125 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la funci贸n y mostrar el resultado\n",
    "top_emojis_df = q2_pandas(file_path)\n",
    "print(top_emojis_df)\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q2_pandas, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q2_pandas - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q2_pandas - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Time, dejando de usar pandas, lectura del archivo completo con la libreria json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busc贸 otro angulo para Q2 Time buscando reducir el tiempo de ejecuci贸n, reemplac茅 la carga de la informaci贸n con Pandas a cargar el archivo y leerlo con la liberia de JSON linea a linea, similar a como se hizo en Q1 Memory pero con el archivo cargado desde antes buscando tener un menor teimpo de ejecuci贸n, as铆 mismo se usa .findall para encontrar los emojis despues de extraer el contenido del tweet de el apartado de content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "    emoji_pattern = re.compile('[\\U0001F600-\\U0001F64F]')  #REGEX para encontrar emojis\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.readlines()\n",
    "        #Carga el json y busca los emojis en cada tweet\n",
    "\n",
    "    for line in data:\n",
    "        tweet = json.loads(line)\n",
    "        content = tweet.get('content', '')\n",
    "        emojis = emoji_pattern.findall(content)\n",
    "        emoji_counter.update(emojis)\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cambio de angulo a Q2 Time ayud贸 considerablemente en la reducci贸n en el tiempo de ejecuci贸n, ahora entre 3.7s y 4s y adicional con una considerable reducci贸n en el pico mas alto de memoria, teniendo como promedio 600MiB versus los 3000MiB de la soluci贸n con Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 7286), ('', 3072), ('', 378), ('', 280), ('', 259), ('', 225), ('', 219), ('', 215), ('', 213), ('', 202)]\n",
      "q2_time - Execution Time: 3.7877962589263916 seconds\n",
      "q2_time - Peak Memory Usage: 605.49609375 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la funci贸n y mostrar el resultado\n",
    "top_emojis = q2_time(file_path)\n",
    "print(top_emojis)\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q2_time, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q2_time - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q2_time - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Memory evitando cargar el archivo completo, se usa lectura linea por linea para buscar los emojis por cada tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la soluci贸n Q2 Memory tom茅 un angulo muy similar al de la soluci贸n final de Q2 Time pero en vez de cargar el archivo JSON completamente se hace lo mismo que en Q1 Memory leyendo el archivo fila a fila, la log铆ca base es la misma que en Q2 Time pero este cambio logra reducir el pico de memoria de la funci贸n a un promedio 550MiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "    emoji_pattern = re.compile('[\\U0001F600-\\U0001F64F]')  #REGEX para encontrar emojis\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            content = tweet.get('content', '')\n",
    "            emojis = emoji_pattern.findall(content)\n",
    "            for emoji in emojis:\n",
    "                emoji_counter[emoji] += 1\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al analizar los tests de eficiencia en tiempo y memoria podemos ver que el tiempo de ejecuci贸n es ligeramente mas alto que en Q2 Time, pero se encuentra una reducci贸n en el pico de memoria, de un promedio de 600MiB a 550MiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 7286), ('', 3072), ('', 378), ('', 280), ('', 259), ('', 225), ('', 219), ('', 215), ('', 213), ('', 202)]\n",
      "q2_memory - Execution Time: 3.8695602416992188 seconds\n",
      "q2_memory - Peak Memory Usage: 551.01953125 MiB\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la funci贸n y mostrar el resultado\n",
    "top_emojis = q2_memory(file_path)\n",
    "print(top_emojis)\n",
    "# Tests de eficiencia de tiempo y memoria\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q2_memory, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q2_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q2_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Time and Memory solucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La soluci贸n de Q3 Time y Q3 Memory comparte mucho del desarrollo de Q2, para variar las soluciones eleg铆 irme con Pandas + REGEX para Q3 Time, con REGEX se puede encontrar las menciones a distintos usuarios para luego guardar los usernames en un dataframe, se reutiliza el .explode que ya se habia explorado en Q2 y se ordena por usuarios mas mencionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Time solucion con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Read the entire file into a Pandas DataFrame\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Regular expression to extract mentions\n",
    "    mention_pattern = re.compile(r'@(\\w+)')\n",
    "\n",
    "    # Extract mentions and explode the DataFrame\n",
    "    df['mentions'] = df['content'].str.findall(mention_pattern)\n",
    "    df_exploded = df.explode('mentions')\n",
    "\n",
    "    # Count and get the top 10 mentions\n",
    "    top_mentions = df_exploded['mentions'].value_counts().head(10)\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    top_mentions_output = list(top_mentions.items())\n",
    "\n",
    "    return top_mentions_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El analisis de los tests de Q3 muestra un tiempo promedio de ejecuci贸n de 9.5s y un pico de uso de memoria de 3400MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1639), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "q3_time - Execution Time: 9.334634780883789 seconds\n",
      "q3_time - Peak Memory Usage: 3332.78125 MiB\n"
     ]
    }
   ],
   "source": [
    "#Executing q3_time and printing the result\n",
    "top_mentions = q3_time(file_path)\n",
    "print(top_mentions)\n",
    "# Timing and memory testing for q1_memory\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q3_time, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q3_time - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q3_time - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Memory solucion, usa el metodo de lectura linea por linea del JSON usado anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la soluci贸n de Q3 Memory se usa una combinaci贸n de REGEX y de lectura fila a fila del JSON de tweets, con una estructura similar a Q2 Memory donde se busca con .findall las menciones dentro de la columna de content, esta soluci贸n reduce en un 45% el pico de consumo de memoria y como se mencion贸 en Q1 ya que la cantidad de tweets procesados no es lo suficientemente alta como para que una lectura linea a linea sea mas lenta que una carga inicial de los datos completos el tiempo de ejecuci贸n tambi茅n se reduce, de 9s en promedio a 4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "    mention_pattern = re.compile(r'@(\\w+)')\n",
    "\n",
    "    # Process file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            mentions = mention_pattern.findall(tweet.get('content', ''))\n",
    "            for mention in mentions:\n",
    "                mention_counter[mention] += 1\n",
    "\n",
    "    # Get the top 10 mentions\n",
    "    top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1639), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "q3_memory - Execution Time: 3.8050408363342285 seconds\n",
      "q3_memory - Peak Memory Usage: 1906.46484375 MiB\n"
     ]
    }
   ],
   "source": [
    "#Executing q3_memory and printing the result\n",
    "top_mentions = q3_memory(file_path)\n",
    "print(top_mentions)\n",
    "# Timing and memory testing for q1_memory\n",
    "start_time = time.time()\n",
    "peak_memory_memory = max(memory_usage((q3_memory, (file_path,))))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"q3_memory - Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"q3_memory - Peak Memory Usage: {peak_memory_memory} MiB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
